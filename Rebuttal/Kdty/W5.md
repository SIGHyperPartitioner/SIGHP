## Response 
Q:  
There is no sequential runtime results for the aforementioned hypergraph computations. Yes, hypergraph partitioning is an important problem but distributed computation may only be justified when the hypergraph does not fit into memory. The datasets used are all small in that sense.

A:  
Thank you for your feedback. I'd like to address your concerns by clarifying three key points:
First, the importance and necessity of distributed processing for hypergraphs are well-established in the literature [1-3].
Second, our experimental setup, including the datasets used, strictly adheres to the previous research on hypergraph partitioning in distributed systems. Notably, the EnWiki dataset is the largest public hypergraph dataset available on open-source platforms such as Stanford SNAP and KONECT.
Third, processing hypergraph algorithms typically requires substantially more memory space than what is needed to simply store the data. It is common for memory usage to be several times larger than the size of the original dataset, sometimes even reaching dozens of times that size.


## Reference 
[1] Heintz B, Hong R, Singh S, et al. MESH: A flexible distributed hypergraph processing system[C]//2019 IEEE International Conference on Cloud Engineering (IC2E). IEEE, 2019: 12-22.  
[2] Jiang W, Qi J, Yu J X, et al. Hyperx: A scalable hypergraph framework[J]. IEEE Transactions on Knowledge and Data Engineering, 2018, 31(5): 909-922.  
[3] Gu Y, Yu K, Song Z, et al. Distributed hypergraph processing using intersection graphs[J]. IEEE Transactions on Knowledge and Data Engineering, 2020, 34(7): 3182-3195.  

## Revision

We have provided a more detailed explanation and revision of the similarity calculation between nodes and partitions.
![](./pic/similarity.png)
